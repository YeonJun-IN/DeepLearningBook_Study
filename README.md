# DeepLearning Book
-------
> Slide - Edited by [Big Leader](http://bigleader.net/) Study <br>
> Reference : [Deep Learning Book](http://www.deeplearningbook.org/) <br>

------



### Contents

#### 1 Introduction
1.1 Who Should Read This Book? <br>
1.2 Historical Trends in Deep Learning <br>

## I Applied Math and Machine Learning Basics

#### 2 Linear Algebra 

2.1 Scalars, Vectors, Matrices and Tensors<br>2.2 Multiplying Matrices and Vectors <br>
2.3 Identity and Inverse Matrices <br>
2.4 Linear Dependence and Span <br>
2.5 Norms <br>
2.6 Special Kinds of Matrices and Vectors <br>
2.7 Eigendecomposition <br>
2.8 Singular Value Decomposition <br>
2.9 The Moore-Penrose Pseudoinverse <br>
2.10 The Trace Operator <br>
2.11 The Determinant <br>
2.12 Example: Principal Components Analysis <br>

#### 3 Probability and Information Theory
3.1 Why Probability? <br>
3.2 Random Variables <br>
3.3 Probability Distributions <br>
3.4 Marginal Probability <br>
3.5 Conditional Probability <br>
3.6 The Chain Rule of Conditional Probabilities <br>
3.7 Independence and Conditional Independence <br>
3.8 Expectation, Variance and Covariance <br>
3.9 Common Probability Distributions <br>
3.10 Useful Properties of Common Functions <br>
3.11 Bayes' Rule <br>
3.12 Technical Details of Continuous Variables <br>
3.13 Information Theory <br>
3.14 Structured Probabilistic Models <br>

#### 4 Numerical Computation
4.1 Overﬂow and Underﬂow <br>
4.2 Poor Conditioning <br>
4.3 Gradient-Based Optimization <br>
4.4 Constrained Optimization <br>
4.5 Example: Linear Least Squares <br>

#### 5 Machine Learning Basics 
5.1 Learning Algorithms <br>
5.2 Capacity, Overﬁtting and Underﬁtting <br>
5.3 Hyperparameters and Validation Sets <br>
5.4 Estimators, Bias and Variance <br>
5.5 Maximum Likelihood Estimation <br>
5.6 Bayesian Statistics <br>
5.7 Supervised Learning Algorithms <br>
5.8 Unsupervised Learning Algorithms <br>
5.9 Stochastic Gradient Descent <br>
5.10 Building a Machine Learning Algorithm <br>
5.11 Challenges Motivating Deep Learning <br>

## II Deep Networks: Modern Practices 

#### 6 Deep Feedforward Networks - [Slide 1](https://www.slideshare.net/KyeongUkJang/chapter-6-deep-feedforward-networks-1) / [Slide 2](https://www.slideshare.net/KyeongUkJang/chapter-6-deep-feedforward-networks-2)
6.1 Example: Learning XOR <br>
6.2 Gradient-Based Learning <br>
6.3 Hidden Units <br>
6.4 Architecture Design <br>
6.5 Back-Propagation and Other DiﬀerentiationAlgorithms <br>
6.6 ~~Historical Notes~~ <br>

#### 7 Regularization for Deep Learning - [Slide 1](https://www.slideshare.net/KyeongUkJang/chapter-7-regularization-for-deep-learning-1) / [Slide 2](https://www.slideshare.net/KyeongUkJang/chapter-7-regularization-for-deep-learning-2) / [Slide 3](https://www.slideshare.net/KyeongUkJang/chapter-7-regularization-for-deep-learning-3)
7.1 Parameter Norm Penalties <br>
7.2 Norm Penalties as Constrained Optimization <br>
7.3 Regularization and Under-Constrained Problems <br>
7.4 Dataset Augmentation <br>
7.5 Noise Robustness <br>
7.6 Semi-Supervised Learning <br>
7.7 Multitask Learning <br>
7.8 Early Stopping <br>
7.9 Parameter Tying and Parameter Sharing <br>
7.10 Sparse Representations <br>
7.11 Bagging and Other Ensemble Methods <br>
7.12 Dropout <br>
7.13 Adversarial Training <br>
7.14 Tangent Distance, Tangent Prop and ManifoldTangent Classiﬁer <br>

#### 8 Optimization for Training Deep Models - [Slide 1](https://www.slideshare.net/KyeongUkJang/chapter-8-optimization-for-training-deep-models)
8.1 How Learning Diﬀers from Pure Optimization <br>
8.2 Challenges in Neural Network Optimization <br>
8.3 Basic Algorithms <br>
8.4 Parameter Initialization Strategies <br>
8.5 Algorithms with Adaptive Learning Rates <br>
8.6 Approximate Second-Order Methods <br>
8.7 Optimization Strategies and Meta-Algorithms <br>

#### 9 Convolutional Networks - [Slide 1](https://www.slideshare.net/KyeongUkJang/chapter-9-convolutional-networks)

9.1 The Convolution Operation<br>9.2 Motivation <br>
9.3 Pooling <br>
9.4 Convolution and Pooling as an Inﬁnitely Strong Prior <br>
9.5 Variants of the Basic Convolution Function <br>
9.6 Structured Outputs <br>
9.7 Data Types <br>
9.8 Eﬃcient Convolution Algorithms <br>
9.9 Random or Unsupervised Features <br>
9.10 The Neuroscientiﬁc Basis for ConvolutionalNetworks <br>
9.11 Convolutional Networks and the History of Deep Learning <br>

#### 10 Sequence Modeling: Recurrent and Recursive Nets - [Slide 1](https://www.slideshare.net/KyeongUkJang/chapter-10-sequence-modeling-recurrent-and-recursive-nets)
10.1 Unfolding Computational Graphs <br>
10.2 ~~Recurrent Neural Networks~~ <br>
10.3 Bidirectional RNNs <br>
10.4 ~~Encoder-Decoder Sequence-to-SequenceArchitectures~~ <br>
10.5 Deep Recurrent Networks <br>
10.6 Recursive Neural Networks <br>
10.7 The Challenge of Long-Term Dependencies <br>
10.8 ~~Echo State Networks~~ <br>
10.9 ~~Leaky Units and Other Strategies for MultipleTime Scales~~ <br>
10.10 ~~The Long Short-Term Memory and Other Gated RNNs~~ <br>
10.11 ~~Optimization for Long-Term Dependencies~~ <br>
10.12 ~~Explicit Memory~~ <br>

#### 11 Practical Methodology - [Slide 1](https://www.slideshare.net/KyeongUkJang/chapter-11-practical-methodology)
11.1 Performance Metrics <br>
11.2 ~~Default Baseline Models~~ <br>
11.3 Determining Whether to Gather More Data <br>
11.4 Selecting Hyperparameters <br>
11.5 ~~Debugging Strategies~~ <br>
11.6 Example: Multi-Digit Number Recognition <br>

#### 12 Applications - [Slide 1](https://www.slideshare.net/KyeongUkJang/chapter-12-applications-1) / [Slide 2](https://www.slideshare.net/KyeongUkJang/chapter-12-applications-2)
12.1 Large-Scale Deep Learning <br>
12.2 Computer Vision <br>
12.3 Speech Recognition <br>
12.4 ~~Natural Language Processing~~ <br>
12.5 Other Applications <br>

## III Deep Learning Research 

#### 13 Linear Factor Models - [Slide 1](https://www.slideshare.net/KyeongUkJang/chapter-13-linear-factor-models) / [Slide 2](https://www.slideshare.net/KyeongUkJang/chapter-13-linear-factor-models-2) 

13.1 Probabilistic PCA and Factor Analysis <br>
13.2 Independent Component Analysis (ICA) <br>
13.3 Slow Feature Analysis <br>
13.4 Sparse Coding <br>
13.5 Manifold Interpretation of PCA <br>

#### 14 Autoencoders - [Slide 1](https://www.slideshare.net/KyeongUkJang/chapter-14-autoencoder) 
14.1 Undercomplete Autoencoders <br>
14.2 Regularized Autoencoders <br>
14.3 Representational Power, Layer Size and Depth <br>
14.4 Stochastic Encoders and Decoders <br>
14.5 Denoising Autoencoders <br>
14.6 Learning Manifolds with Autoencoders <br>
14.7 ~~Contractive Autoencoders~~ <br>
14.8 ~~Predictive Sparse Decomposition~~ <br>
14.9 Applications of Autoencoders <br>

#### 15 Representation Learning - [Slide 1](https://www.slideshare.net/KyeongUkJang/chapter-15-representation-learning-1) / [Slide 2](https://www.slideshare.net/KyeongUkJang/chapter-15-representation-learning-2)
15.1 Greedy Layer-Wise Unsupervised Pretraining <br>
15.2 Transfer Learning and Domain Adaptation <br>
15.3 Semi-Supervised Disentangling of Causal Factors <br>
15.4 Distributed Representation <br>
15.5 ~~Exponential Gains from Depth~~ <br>
15.6 ~~Providing Clues to Discover Underlying Causes~~ <br>

#### 16 Structured Probabilistic Models for Deep Learning - [Slide 1](https://www.slideshare.net/KyeongUkJang/chapter-16-structured-probabilistic-models-for-deep-learning-1) / [Slide 2](https://www.slideshare.net/KyeongUkJang/chapter-16-structured-probabilistic-models-for-deep-learning-2-137924213)
16.1 The Challenge of Unstructured Modeling  <br>
16.2 ~~Using Graphs to Describe Model Structure~~ <br>
16.3 Sampling from Graphical Models <br>
16.4 Advantages of Structured Modeling <br>
16.5 Learning about Dependencies <br>
16.6 Inference and Approximate Inference <br>
16.7 The Deep Learning Approach to Structured Probabilistic Models <br>

#### 17 Monte Carlo Methods - [Slide 1](https://www.slideshare.net/KyeongUkJang/chapter-17-monte-carlo-methods)
17.1 Sampling and Monte Carlo Methods <br>
17.2 Importance Sampling <br>
17.3 Markov Chain Monte Carlo Methods <br>
17.4 Gibbs Sampling <br>
17.5 The Challenge of Mixing between Separated Modes <br>

#### 18 Confronting the Partition Function - 
18.1 The Log-Likelihood Gradient <br>
18.2 Stochastic Maximum Likelihood and Contrastive Divergence <br>
18.3 Pseudolikelihood <br>
18.4 Score Matching and Ratio Matching <br>
18.5 Denoising Score Matching <br>
18.6 Noise-Contrastive Estimation <br>
18.7 Estimating the Partition Function <br>

#### 19 Approximate Inference
19.1 Inference as Optimization <br>
19.2 Expectation Maximization <br>
19.3 MAP Inference and Sparse Coding <br>
19.4 Variational Inference and Learning <br>
19.5 Learned Approximate Inference <br>

#### 20 Deep Generative Models

20.1 Boltzmann Machines <br>
20.2 Restricted Boltzmann Machines <br>
20.3 Deep Belief Networks <br>
20.4 Deep Boltzmann Machines <br>
20.5 Boltzmann Machines for Real-Valued Data <br>
20.6 Convolutional Boltzmann Machines <br>
20.7 Boltzmann Machines for Structured or Sequential Outputs <br>
20.8 Other Boltzmann Machines <br>
20.9 Back-Propagation through Random Operations <br>
20.10 Directed Generative Nets <br>
20.11 Drawing Samples from Autoencoders <br>
20.12 Generative Stochastic Networks <br>
20.13 Other Generation Schemes <br>
20.14 Evaluating Generative Models <br>
20.15 Conclusion <br>

#### Bibliography

### Other Session

- NLP - [Slide1](https://www.slideshare.net/KyeongUkJang/natural-language-processingnlp-basic) / [Slide2](https://www.slideshare.net/KyeongUkJang/natural-language-processingnlp-basic-2)
- 
